{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xso5Q-Lbynic"
      },
      "source": [
        "# Assignment 5: Model Based Reinforcement Learning\n",
        "\n",
        "### Due Date: \n",
        "May 2 at 11:59 PM \n",
        "\n",
        "### Writeup: \n",
        "https://docs.google.com/document/d/1x51KdYtHO9RggO0lrtlkFH_55yGFfyEK0PnLMqDzJtA/edit?usp=sharing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDGlWqE5lGsU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "Welcome to Assignment 5 of CS 4756/5756. In this assignment, you will explore the effects of model based reinforcement learning, while getting familiar with Stable Baselines, a popularly used reinforcement learning library. \n",
        "\n",
        "You will use the MountainCar-v0 environment for this assignment. Refer to the Gym website for more details about the [MountainCar environment](https://gymnasium.farama.org/environments/classic_control/mountain_car/).\n",
        "\n",
        "\n",
        "**Structure of Assignment:** \n",
        "This assignment is built up by the following components: \n",
        "- Setting Up: dependency installing and initializations \n",
        "- Helper Functions: A set of provided helper functions for you to evaluate and visualize policies. \n",
        "- Part 1: Introduction to StableBaselines3 package. You will implement some helper classes and write some scripts to train a PPO agent with StableBaselines3. \n",
        "- Part 2: Collect data on the MountainCar environment transition and train a world model \n",
        "\n",
        "*Note: Bulk of implementation is in Part 1 and 2, building your foundation of helper functions and classes to use in Part 3.*\n",
        "\n",
        "- Part 3: Train a learner PPO policy on the environment modeled in part 1 as world environment. \n",
        "- [EC] Part 4: Collect new data with learner policy and aggregate with expert collected data, repeat part 2 world model training and part 3 agent training. \n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully. \n",
        "\n",
        "\n",
        "**Getting Started:** You are free to complete this assignment on **either  [Google Colab](https://colab.research.google.com/) or your local machine**. Note that there will be a small amount of extra setup if you choose to complete the assignment on your local machine (see **Setup** section below).\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\n",
        "As mentioned above, you are free to use Google Colab or your local machine for this assignment. Regardless of your choice, **you will need to run the cell below**. If you are using your local machine, though, you will first need to set up a conda environment that contains the packages found in requirements.txt.\n",
        "\n",
        "\n",
        "### Setting Up Conda Environment (Local Machine Only)\n",
        "\n",
        "In order to complete the assignment locally, you will need to install the required libraries. To do this, we will use the package manager [Conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).\n",
        "\n",
        "* First, create a Conda environment in the terminal with the correct version of python by running: `conda create --name cs4756_a5 python=3.10`\n",
        "* Next, activate the environment by running: `conda activate cs4756_a5`\n",
        "* Lastly, install the required libraries by runnning: `pip install -r requirements.txt`\n",
        "\n",
        "When you run the notebook, make sure to set the Python interpreter and kernel to be the version of python from the `cs4756_a5` environment. If you are using VSCode, you may need to restart after creating the environment in order for `cs4756_a5` to be a visible option that you can select for your kernel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHOfoL9J87BJ",
        "outputId": "1043e089-81a9-47dc-ca28-89ac1a17b53c"
      },
      "outputs": [],
      "source": [
        "# Set Up:\n",
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !pip install -U renderlab\n",
        "    !pip install -U colabgymrender\n",
        "    !pip install -U moviepy==0.2.3.5\n",
        "    !pip install imageio==2.4.1\n",
        "    !pip install --upgrade AutoROM\n",
        "    !AutoROM --accept-license\n",
        "    !pip install gymnasium\n",
        "    !pip install gym[classic_control] > /dev/null 2>&1\n",
        "    !pip install stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "seed = 24\n",
        "data_seed = 700\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpTyu7Ap8_jA"
      },
      "source": [
        "## [PROVIDED] Helper Functions\n",
        "\n",
        "### Reseed \n",
        "\n",
        "We provide the reseeding function that will update the seed to a given customary seed passed in as parameter. This helps reproducibility of the code and hopes to eliminate confusion on achieved number results for the rest of this assignment. \n",
        "\n",
        "An optional parameter of `env` is passed in, such that when one is provided, the function will also set the seed of the random number generator of that particular gym environment.\n",
        "\n",
        "**Note**: All calls to this function will be given available to you for the rest of the assignment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed, env=None):\n",
        "    '''\n",
        "        Sets the seed for reproducibility \n",
        "\n",
        "        When @param env is provided, also sets the \n",
        "        random number generataor of the gym environment \n",
        "        to this particular seed\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    if env is not None: \n",
        "        env.unwrapped._np_random = gym.utils.seeding.np_random(seed)[0]\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize\n",
        "\n",
        "Below, we provide the helper function `visualize` for your use. This function will create a visualization of your specified environment by running an episode with the provided algorithm. If you are using Colab, calling this function will render the visualization within the notebook. If you are using your local machine, this function will instead save a video of the visualization to your current directory (rendering videos in Jupyter Notebooks is not widely supported outside of Colab). \n",
        "\n",
        "**Note** that in this code, a choice is provided on whether to vectorize the environment. The difference across vectorized and not vectorized gymnasium environments will be explained in the StableBaselines Introduction section. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aQv91DL9F-_"
      },
      "outputs": [],
      "source": [
        "def visualize(env_name='MountainCar-v0', algorithm=None, video_name=\"test\", env_args={}):\n",
        "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            env_name: Name of the gym environment to roll out `algorithm` in, it will be instantiated using gym.make or make_vec_env\n",
        "            algorithm (PPOActor): Actor whose policy network will be rolled out for the episode. If\n",
        "            no algorithm is passed in, a random policy will be visualized.\n",
        "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
        "            when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.select_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        from renderlab import RenderFrame\n",
        "\n",
        "        directory = './video'\n",
        "        env_args['render_mode'] = 'rgb_array'\n",
        "        env = gym.make(env_name, **env_args)\n",
        "        env = RenderFrame(env, directory)\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for i in range(200):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, done, truncate, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.play()\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "\n",
        "        env_args['render_mode'] = 'rgb_array'\n",
        "        env = gym.make(env_name, **env_args)\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            res = env.step(action)\n",
        "            obs, reward, done, truncate, info = res\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            im = env.render()\n",
        "            im = im[:,:,::-1]\n",
        "\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Evaluation function\n",
        "\n",
        "The `evaluate_policy` function takes an agent actor, an environment whose output observations can be directly applied to the actor, and evaluates the policy by doing the following: \n",
        "\n",
        "- Rollout actor for a default of 100 trajectories, and record the total reward\n",
        "- Return the average trajectory rewards over these episodes. \n",
        "\n",
        "**Note**: since the actor we will be defining in this assignment exclusively uses a StableBaselines3 PPO agent, then the environment provided must be an instance of `VecEnv`, more information introduced in Part 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(actor, environment, num_episodes=100, progress=True): \n",
        "    '''\n",
        "        Returns the mean trajectory reward of rolling out `actor` on `environment \n",
        "\n",
        "        Parameters \n",
        "        - actor: PPOActor instance, defined in Part 1 \n",
        "        - environment: classstable_baselines3.common.vec_env.VecEnv instance \n",
        "        - num_episodes: total number of trajectories to collect and average over\n",
        "    '''\n",
        "    total_rew = 0 \n",
        "\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "    for _ in iterate: \n",
        "        obs = environment.reset() \n",
        "        done = False\n",
        "\n",
        "        while not done: \n",
        "            action = actor.select_action(obs)\n",
        "            \n",
        "            next_obs, reward, done, info = environment.step(action) \n",
        "            total_rew += reward\n",
        "            \n",
        "            obs = next_obs \n",
        "    \n",
        "    return (total_rew / num_episodes).item() \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgJQoJ69QKS"
      },
      "source": [
        "## Introduction to the Mountain Car Environment\n",
        "\n",
        "MountainCar-v0 is a classic control problem in the field of reinforcement learning. The task is to drive the cart up the hill and to the goal on the right. The environment consists of a cart that can move along a track that changes in elevation, starting a various possible positions on this track. The goal is to reach the top of hte hill. \n",
        "\n",
        "The observation space consists of two variables: position of the car along the x-axis, and the velocity of the car. The action space includes three discrete actions: accelerate the cart to the left, do not accelerate, and accelerate the cart to the right. An episode ends when the cart reaches the top or when it has taken over 200 steps.\n",
        "\n",
        "**Run the cell below to visualize the MountainCar-v0 environment with a random policy:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "U75gc8b-9qD0",
        "outputId": "2d7c5876-3178-4b3c-b14c-b112b60b5c44"
      },
      "outputs": [],
      "source": [
        "visualize(env_name='MountainCar-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvDW2CKcyssI"
      },
      "source": [
        "## Part 1: Train an expert using StableBaselines3 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh6-BcrDk5Kt"
      },
      "source": [
        "### Overview: Introduction to Stable Baselines 3\n",
        "\n",
        "**Please Read Carefully**: Contains information necessary for following implementations. \n",
        "\n",
        "StableBaselines3 is popular off-the-shelf set of reliable implementations of reinforcement learning algorithms in PyTorch. \n",
        "\n",
        "In this assignment, we will be using its PPO (Proximal Policy Gradient) implementation as our agent. \n",
        "\n",
        "Each of the algorithm implementation is a subclass of the `stable_baselines3.common.base_class.BaseAlgorithm` class, which provides us with the following functions: \n",
        "\n",
        "- `learn(total_timesteps, callback=None, log_interval=100, tb_log_name='run', reset_num_timesteps=True, progress_bar=False)`: This is the training loop of any of the RL algorithm implementations. Training is done by calling this function with an appropriate amount of `total_timesteps` \n",
        "- `predict(observation)`: Returns a tuple `(predicted_action, next_hidden_state)` based on input `observation`. If we are not using an RNN, the next hidden state can be neglected. \n",
        "- `save(path)`: Saves the current policy parameters into a `.zip` file with given `path`. Note that the `path` does not have the `.zip` postfix. \n",
        "- `load(path, env=None)`: Loads a saved a `.zip` checkpoint into this RL implementation model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF7g5LGEn5iH"
      },
      "source": [
        "### [PROVIDED] Hyperparameters\n",
        "\n",
        "There are a set of hyperparameters that can be tuned toward a better performance, for the sake of simplicity, we will provide the hyperparameters for the StableBaselines3 PPO implementation. The main ones we specify include the following: \n",
        "* `n_steps`: the number of steps to run with the environment for each update to the policy network\n",
        "* `net_arch`: The network architecture of the policy network and the critic network: \n",
        "  * `pi`: a list that specifies the hidden dimensions of the policy network. The input and output dimension are determined by the environment associated with this policy \n",
        "  * `vf`: a list that specifies the hidden dimensions of the critic network. \n",
        "  * `activation_fn`: Nonlinearity to be applied between each of the MLP layers \n",
        "\n",
        "For a more comprehensive list and description of each of these hyperparameters, visit the official [documentation page](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters) for more information. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwMlQmYQmu2e"
      },
      "outputs": [],
      "source": [
        "# Dependencies:\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
        "\n",
        "hyperparameters = {\n",
        "    \"n_steps\": 10000,\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": {\n",
        "            \"pi\": [32, 64, 32],\n",
        "            \"vf\": [32, 64, 32],\n",
        "            \"activation_fn\": \"tanh\",\n",
        "        }\n",
        "    },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1: Vectorized Environmnent (2 points)\n",
        "\n",
        "#### Overview\n",
        "For any StableBaselines3 algorithm implementation, any gymnasium environment used need to be converted into a [vectorized environment](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#) of `VecEnv` type. \n",
        "\n",
        "A vectorized environment stacks multiple independent environments into one, stepping multiple `n` environments each time. If we set the the `n_envs` parameter to 3, then 3 environments will be stepped each time the VecEnv is stepped. \n",
        "\n",
        "**For the rest of this assignment, all vectorized environments with n_env=n will be described as n-vectorized.** \n",
        "\n",
        "With a vectorized environment that steps multiple environments at the same time, the model learning process can be made more efficient through parallelization trajectory collection across these independent environments. This `n_envs` parameter can be tailored to the specific machines.  \n",
        "\n",
        "**Import Note of Difference**: \n",
        "- The vectorized environments now require input action to be a shape of `n_envs * act_dim`. The output observation from `step` and `reset` will also have the shape of `n_envs * obs_dim`\n",
        "- The VecEnv `reset()` function returns only the observation, while the gymnasium.Env `reset()` function returns a tuple `(observation, info_dict)` \n",
        "- The `vec_env.step(action)` function returns a 4-tuple of `(obs, reward, terminated, info)`, while the `gym_env.step(action)` returns a 5-tuple of `(obs, reward, terminated, truncated, info)`. The `terminated` value from VecEnv would equivalent to the gymnasium environment's `terminated or truncated` \n",
        "\n",
        " \n",
        "A VecEnv instance can be created using the `make_vec_env` function, which takes the id of the wanted gymnasium environment, as well as the number of environments needed. This function has the following key parameters \n",
        "- `env_id`: required parameter that is the id of the gymnasium environment, or instantiated gym environment, or a callable that returns an env. \n",
        "- `n_envs`: The number of environments to have in parallel \n",
        "- `seed`: The initial seed for the random number generator \n",
        "- `env_kwargs`: An optional parameter to pass into the environment constructor. \n",
        "\n",
        "More detailed function documentation can be found in this [page](https://stable-baselines3.readthedocs.io/en/master/common/env_util.html#stable_baselines3.common.env_util.make_vec_env). \n",
        "\n",
        "#### Instructions \n",
        "For this part, please create two vectorized version of MountainCar-v0 with 3 and 1 environments stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# TODO: \n",
        "real_vec_env_3 = None\n",
        "real_vec_env_1 = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgB9tsEkrhji"
      },
      "source": [
        "### 1.2: Actor Definition (3 points)\n",
        "\n",
        "**Instruction**: You will need to implement the following PPOActor class, which serves as a wrapper to provide PPO model predictions. \n",
        "- `__init__`: Takes a path to the checkpoint and the corresponding environment, and load an instance of this PPO checkpoint. However if a PPO model is given, then the internally representing model uses that directly instead. This is for use in the Callback function, and since we provide that implementation for you, you will only need to implement the model loading portion of the constructor. \n",
        "\n",
        "**Note**: the environment being passed is usually be the 3-vectorized corresponding environment later in the program, but they will not be used by the actor other than initializing the model. \n",
        "  \n",
        "- `select_action`: Takes an observation and produce the corresponding action prediction from the checkpoint PPO model. While implementing, take note of the output of the `predict` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI4B4Ji8rgz5"
      },
      "outputs": [],
      "source": [
        "class PPOActor():\n",
        "    def __init__(self, ckpt: str=None, environment: VecEnv=None, model=None):\n",
        "        '''\n",
        "          Requires environment to be a 1-vectorized environment\n",
        "\n",
        "          The `ckpt` is a .zip file path that leads to the checkpoint you want \n",
        "          to use for this particular actor.  \n",
        "          \n",
        "          If the `model` variable is provided, then this constructor will store\n",
        "          that as the internal representing model instead of loading one from the \n",
        "          checkpoint path\n",
        "          \n",
        "        '''\n",
        "        assert ckpt is not None or model is not None\n",
        "        if model is not None: \n",
        "            self.model = model \n",
        "            return \n",
        "        \n",
        "        # TODO: MODIFY\n",
        "        self.model = None\n",
        "        # End TODO \n",
        "         \n",
        "    \n",
        "    def select_action(self, obs):\n",
        "        '''\n",
        "          Gives the action prediction of this particular actor \n",
        "        '''\n",
        "        # TODO:\n",
        "        return None\n",
        "        # END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYVvwAJn1O2"
      },
      "source": [
        "### 1.3: [PROVIDED] Callbacks\n",
        "\n",
        "To visualize the training process, since it could take a significant amount of time, StableBaselines3 provides a mean for us to visualize the training progress through a BaseCallback class instance, which can be optionally passed in as a parameter of the `learn` function. This Callback function is customizable by defining a subclass of `BaseCallback`. \n",
        "\n",
        "For this part, we provide you with a customized callback that evaluates the model under training every 120000 steps on an evaluating environment, which will be the 1-vectorized environment you have instantiated in the previous portion. Based on this evaluation result, this callback will save a checkpoint of the model if it is, so far, the best performing model. At the end of training, a plot of all evaluation results with respect to number of steps will be generated. \n",
        "\n",
        "You are free to modify this callback class to help you visualize training in any way most convenient for you, but is **NOT REQUIRED**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2lI5ctYnLI4"
      },
      "outputs": [],
      "source": [
        "class PPOCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0, save_path='default', eval_env=None):\n",
        "        super(PPOCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "\n",
        "        self.save_freq = 120000\n",
        "        self.min_reward = -np.inf\n",
        "        self.actor = None\n",
        "        self.eval_env = eval_env \n",
        "        \n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.eval_steps = []\n",
        "        self.eval_rewards = [] \n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        pass \n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.actor = PPOActor(model=self.model)\n",
        "\n",
        "    def _on_rollout_start(self) -> None: \n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass \n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        episode_info = self.model.ep_info_buffer\n",
        "        rewards = [ep_info['r'] for ep_info in episode_info]\n",
        "        mean_rewards = np.mean(rewards)\n",
        "\n",
        "        self.rewards.append(mean_rewards)\n",
        "        \n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        if self.eval_env is None: \n",
        "            return True \n",
        "\n",
        "        if self.num_timesteps % self.save_freq == 0 and self.num_timesteps != 0: \n",
        "            mean_reward = evaluate_policy(self.actor, environment=self.eval_env, num_episodes=20) \n",
        "            print(f'evaluating {self.num_timesteps=}, {mean_reward=}=======')\n",
        "\n",
        "            self.eval_steps.append(self.num_timesteps)\n",
        "            self.eval_rewards.append(mean_reward)\n",
        "            if mean_reward > self.min_reward: \n",
        "                self.min_reward = mean_reward \n",
        "                self.model.save(self.save_path)\n",
        "                print(f'model saved on eval reward: {self.min_reward}')\n",
        "        \n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        plt.plot(self.eval_steps, self.eval_rewards, c='red')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Rewards over Episodes')\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy2goV7lPAc4"
      },
      "source": [
        "### 1.4 PPO Expert Initialization and Training (2 points)\n",
        "\n",
        "#### Overview\n",
        "\n",
        "The `stable_baselines3.ppo.PPO` class inherits from the `BaseAlgorithm` class described at the beginning of this section, and is specifically implemented for the PPO algorithm. To initialize a class, the following parameters are especially important: \n",
        "- `policy: str`: The policy type we use to train the agent, common ones include MlpPolicy and CnnPolicy. In our case, we will be using the MlpPolicy. \n",
        "- `env: VecEnv`: The environment that the agent rollouts on for training, must be vectorized or it will be vectorized by the PPO implementation \n",
        "- `n_steps`: number of steps to optimize the policy for \n",
        "- Other hyperparameters specified in the `hyperparameters` dictionary we provided, can be directly applied using the \\*\\* operator. \n",
        "\n",
        "#### Instructions \n",
        "- Initialize a PPO MLP policy as expert, using the 3-env VecEnv initialized in the previous part and pass in the given hyperparameters. \n",
        "- Train the expert with an instance of the `PPOCallback` defined before. No need to save the resulting model into checkpoint since that is done for you in the Callback class\n",
        "    \n",
        "    (HINT): Look at the beginning of Part 1 for useful functions for training. \n",
        "\n",
        "\n",
        "**Estimated Training Time**: \n",
        "- 10 minutes on a Mac M1 CPU\n",
        "- 30 minutes on Google Colab CPU "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOFH-zYzPAc5",
        "outputId": "fa966b97-507c-4a45-bac0-0755b3636b1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "ckpt_path = 'expert'\n",
        "total_steps = 1500000\n",
        "\n",
        "reseed(seed) \n",
        "expert_callback = PPOCallback(save_path=ckpt_path, eval_env=real_vec_env_1)\n",
        "\n",
        "# TODO\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5: Evaluate Expert (2 points)\n",
        "\n",
        "Initialize an expert PPOActor instance and evaluate the expert agent using the `evaluate_policy` function on the real environment. \n",
        "\n",
        "**Expected Reward**: Around -130 to -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: MODIFY\n",
        "expert = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVfp16qPAc5"
      },
      "source": [
        "### 1.6: [PROVIDED] Visualize Expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "6g4RRIu1PAc5",
        "outputId": "52b1844d-9067-4a22-a658-14e205e90951"
      },
      "outputs": [],
      "source": [
        "visualize(algorithm=expert, video_name='expert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA9VUmpXPAc5"
      },
      "source": [
        "## Part 2: Collect Data and Train World Model\n",
        "\n",
        "### Overview: \n",
        "Unlike in simulation, we can rarely obtain the full transition function of real world scenarios, and we emulate that property in this assignment here. \n",
        "\n",
        "Assuming we do not have the underlying logic to the MountainCar-v0, given that we have an expert agent in solving this particular problem, we take the following modeled based reinforcement learning approach to learn an RL agent that can be applied to the real scenario. \n",
        "\n",
        "In real life, we might not have such a trained expert, and human operating the robot remotely could be one source of expert data. \n",
        "\n",
        "1. Rollout a series of expert trajectories in the true environment (analogous to collecting a set of human demonstrations on the robot) \n",
        "2. Define and train a world model with the trajectory transitions as input data\n",
        "3. Define a new environment that applies the trained world model\n",
        "4. Learn an RL agent under the learned environment\n",
        "5. Evaluate this agent using the real environment\n",
        "\n",
        "### Instructions\n",
        "\n",
        "You will need to implement the following functions and classes \n",
        "- `data_collect`: a helper function that rolls out a policy on an environment, and returning a tuple of lists representing the transitions \n",
        "- `WorldModel` : a `torch.nn` module defining the architecture of the world. \n",
        "- `train_world_model` and `eval_world_model`: Training and evaluation loop of the world model\n",
        "\n",
        "Follow the instructions below to implement each of these components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajJw4tvPPAc5"
      },
      "source": [
        "### 2.1: Collect Data\n",
        "\n",
        "#### data_collect function (6 points)\n",
        "**Instructions**: \n",
        "\n",
        "The `data_collect` function should rollout a policy actor on the environment for a total of `num_steps`, with a maximum trajectory length of `traj_max_length`, then returning 3 lists: `observations`, `actions`, `next_observations` such that for any transition $i \\leq$ num_steps: \n",
        "\n",
        "data_env with intiial state `observations[i]`, when stepped with `actions[i]`, yields a new state `next_observations[i]`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynngHBnZPAc5"
      },
      "outputs": [],
      "source": [
        "def data_collect(num_steps: int, traj_max_length: int, data_env: gym.Env, actor: PPOActor):\n",
        "    '''\n",
        "    Collects observation, action, next_observation triplet data for `num_trajectories`\n",
        "    each with a maximimum step count of `traj_max_length`\n",
        "\n",
        "    - num_steps: Number of total steps to collect data over, should also be the sum of trajectory lengths\n",
        "    - traj_max_length: Maximum length of each trajectory\n",
        "    - data_env: The environment to collect data under, NOT A VECENV\n",
        "\n",
        "    - actor: A function that takes a `data_env` observation as input and outputs an action admissible to `data_env`\n",
        "\n",
        "    Returns: (observations, actions, next_observations), each being a list\n",
        "    '''\n",
        "    \n",
        "    observations, actions, next_obs = [], [], []\n",
        "    \n",
        "    # TODO: \n",
        "\n",
        "    # END TODO\n",
        "    return observations, actions, next_obs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Provided] function that visualizes collected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_collected_data(observations, next_obs): \n",
        "    '''\n",
        "        Takes the first 300 data points and generates a \n",
        "        plot of the observations and next_obs. \n",
        "    '''\n",
        "    print(f'Dataset Size: {len(observations)}')\n",
        "    plt.close()\n",
        "    plt.plot(np.arange(300), observations[:300], c='blue')\n",
        "    plt.plot(np.arange(300), next_obs[:300], c='red')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run Data Collection (1 point) \n",
        "\n",
        "**Instruction**: \n",
        "Run data collection function on the real policy with expert policy trained in part 1. \n",
        "\n",
        "**Note**: the `data_collect` function requires the environment provided to be a regular gymnasium environment instead of a vectorized environment. Please make sure to not confuse it with `real_vec_env_1` defined in part 1.1. \n",
        "\n",
        "**Note**: Here is a list of currently created environments after running this following cell: \n",
        "- `real_vec_env_1` \n",
        "- `real_vec_env_3` \n",
        "- `real_env` \n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "LcvLC7IatH1l",
        "outputId": "bf8c8ce4-4834-4098-c8e6-36c9065ecab1"
      },
      "outputs": [],
      "source": [
        "total_steps = 100000\n",
        "traj_max_length = 500\n",
        "\n",
        "real_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
        "\n",
        "reseed(data_seed, env=real_env) \n",
        "\n",
        "# TODO: MODIFY to run data collect function to collect data\n",
        "observations, actions, next_obs = None\n",
        "# END TODO \n",
        "\n",
        "visualize_collected_data(observations, next_obs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[PROVIDED]: Defining the torch dataset and dataloader used for training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKLqI54LPAc5",
        "outputId": "b4e21497-68f5-4d30-b31e-800c14ae9b7e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class WorldDataset(Dataset):\n",
        "    def __init__(self, obs, actions, next_obs):\n",
        "        self.obs = obs\n",
        "        self.actions = actions\n",
        "        self.next_obs = next_obs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.obs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'orig_obs': self.obs[idx],\n",
        "            'action': self.actions[idx],\n",
        "            'next_obs': self.next_obs[idx]\n",
        "        }\n",
        "\n",
        "split = len(observations) // 5\n",
        "\n",
        "val_data = WorldDataset(observations[:split], actions[:split], next_obs[:split])\n",
        "train_data = WorldDataset(observations[split:], actions[split:], next_obs[split:])\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=128)\n",
        "val_dataloader = DataLoader(val_data, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEDyy3aaPAc5"
      },
      "source": [
        "### 2.2: Define World Model (3 points)\n",
        "\n",
        "The `WorldModel` class should define a neural network that takes a state-action pair and outputs a state in the state space. The network should have the following architecture. \n",
        "\n",
        "- Layer 1: a fully-connected layer with `inp_dim` input nodes and `hidden_dim_1` output nodes, followed by a ReLU activation function.\n",
        "- Layer 2: a fully-connected layer with `hidden_dim_1` input nodes and `hidden_dim_2` output nodes, followed by a ReLU activation function.\n",
        "- Output layer: a fully-connected layer with `hidden_dim_2` input nodes and `output_dim` output nodes. \n",
        "\n",
        "The `forward` function should take two inputs: `state` and `action`, concatenate them along the last dimension, and then pass it through the model architecture. For instance, if the state has shape `n * s`, and the action has shape $n \\times a$, then the input to the model should be `n * (s + a)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgJgF-_2PAc5"
      },
      "outputs": [],
      "source": [
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
        "        super(WorldModel, self).__init__()\n",
        "        self.input_dim = input_dim \n",
        "\n",
        "        # TODO: \n",
        "\n",
        "        # END TODO  \n",
        "\n",
        "    def forward(self, state, action):\n",
        "        '''\n",
        "            Expected `state` to have shape n * s_dim \n",
        "            Expected `action` to have shape n * a_dim\n",
        "        '''\n",
        "        n, s_dim = state.shape \n",
        "        n_a, a_dim = action.shape \n",
        "        assert n == n_a\n",
        "        assert s_dim + a_dim == self.input_dim\n",
        "        \n",
        "        # TODO: MODIFY\n",
        "        return None\n",
        "        # END TODO\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHd3cSEiPAc5"
      },
      "source": [
        "### 2.3: Define Training and Validation Function for World Model (12 points, 6 each): \n",
        "\n",
        "The `train_world_model` function should train the provided model for one epoch, using the optimizer and criterion provided on the given train_dataloader. This function should iterate through each batch of the `train_dataloader` once, update the world model based on the loss calculated by criterion, then step the optimizer. \n",
        "\n",
        "The `eval_world_model` function is similar to the `train_world_model` function with iteration through the batches of the `eval_dataloader` and computes the loss using the given criterion. Note that no update to model should be made and gradients should not be calculated during the forward pass. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_world_model(model, optimizer, criterion, train_dataloader):\n",
        "\n",
        "    '''\n",
        "        This function should train the torch model `model` using the \n",
        "        optim `optimizer` and `criterion` as loss function, on one pass \n",
        "        of the `train_dataloader` \n",
        "\n",
        "        This is should train the model for on epoch, as in one pass through\n",
        "        the training data. \n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset. \n",
        "    '''\n",
        "    # TODO: \n",
        "\n",
        "    # END TODO\n",
        "\n",
        "def eval_world_model(model, criterion, eval_dataloader):\n",
        "    '''\n",
        "        This function should evaluate the torch model `model` using \n",
        "        `criterion` as loss function, on one pass of the `eval_dataloader` \n",
        "\n",
        "        This is should evaluate the model on the validation dataset. \n",
        "\n",
        "        Take note that during evaluation, the model should not be updated \n",
        "        in any way and gradients should not be calculated. \n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset. \n",
        "\n",
        "    '''\n",
        "    # TODO: \n",
        "\n",
        "    # END TODO: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4: Train the World Model (3 points)\n",
        "\n",
        "Train an instance of `WorldModel` for 50 epochs with the dataloader built in previous section, using Adam optimizer and MSE loss, with an lr of 0.0001. \n",
        "\n",
        "Provide a plot of training and evaluation losses with respect to training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OoJ814CmPAc5",
        "outputId": "a912ba14-65f0-497f-8c4c-c8ca73eca53a"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "reseed(seed)\n",
        "\n",
        "world_model = world_model = WorldModel(input_dim=3, hidden_dim_1=16, hidden_dim_2=128, output_dim=2)\n",
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(world_model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "# TODO\n",
        "# END TODO: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn9hLKeqPAc6"
      },
      "source": [
        "### 2.5: [PROVIDED] Build Gym Environment with World Model\n",
        "\n",
        "The following `WorldModelEnv` class is largely defined for you to train your next PPO agent as the reinforcement learning component of MBRL. \n",
        "\n",
        "In this environment, the reward is calculated the same as the MountainCar-v0 gymnasium environment, with a -1 for each step that the car has not reached the goal. \n",
        "\n",
        "\n",
        "**To Initialize** a `WorldModelEnv` environment, a `world_model` (an instance of WorldModel in this case) should be passed in as argument, which will be used as the transition function in the `step()` function. \n",
        "\n",
        "This environment is registered with an id of **WorldModelMountainCar**, which can be initialized using `gym.make` or directly initializing it. \n",
        "\n",
        "\n",
        "**Run the following cell to define and register this environment** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVRWMZ-BPAc6"
      },
      "outputs": [],
      "source": [
        "class WorldModelEnv(gym.Env):\n",
        "    def __init__(self, world_model: WorldModel, render_mode: str='rgb_array'):\n",
        "        super(WorldModelEnv, self).__init__()\n",
        "        self.world_model = world_model\n",
        "        self.corr_env = gym.make('MountainCar-v0', render_mode=render_mode).unwrapped\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(3)\n",
        "        self.observation_space = gym.spaces.Box(low=np.array([-1.2,  -0.07]), high=np.array([0.6, 0.07]), shape=(2,), dtype='float32')\n",
        "        self.metadata = {\n",
        "            'render_modes': ['human', 'rgb_array']\n",
        "        }\n",
        "        self.render_mode = 'rgb_array'\n",
        "\n",
        "        self.state = np.array([np.random.rand(1).item() * 0.2 - 0.6, 0])\n",
        "\n",
        "        self.min_action, self.max_action = -1.0, 1.0\n",
        "        self.goal_position = 0.45\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "        \n",
        "    def seed(self, seed=None):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.state = np.array([np.random.rand(1).item() * 0.2 - 0.6, 0])\n",
        "        self.step_count = 0\n",
        "        self.corr_env.reset()\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        with torch.no_grad():\n",
        "            state = torch.from_numpy(self.state).float().unsqueeze(0) \n",
        "            action = torch.from_numpy(np.array([action])).unsqueeze(0)\n",
        "            next_state = self.world_model(state, action)\n",
        "            next_state = np.array(next_state.squeeze(0))\n",
        "            self.state = np.clip(next_state, a_min=np.array([-1.2, -0.07]), a_max=np.array([0.6, 0.07]))\n",
        "        self.step_count += 1\n",
        "\n",
        "        terminated = bool(self.state[0] >= self.goal_position)\n",
        "\n",
        "        # truncate = False\n",
        "        truncate = bool(self.step_count > 2999)\n",
        "        reward = -1\n",
        "        return self.state, reward, terminated, truncate, {}\n",
        "\n",
        "    def render(self):\n",
        "        self.corr_env.state = (self.state[0], self.state[1])\n",
        "        returned = self.corr_env.render()\n",
        "        return returned\n",
        "\n",
        "\n",
        "gym.register(id='WorldModelMountainCar', entry_point=WorldModelEnv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP3oVCNTPAc6"
      },
      "source": [
        "### 2.6: Visualize World Model with Trained Expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "ndcsAZKuPAc6",
        "outputId": "5c331f2b-d250-43f1-9572-7f2b42d08ccf"
      },
      "outputs": [],
      "source": [
        "visualize(\n",
        "    env_name='WorldModelMountainCar',\n",
        "    algorithm=expert,\n",
        "    video_name='expert_trained_world',\n",
        "    env_args={\n",
        "        'world_model': world_model\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Train a PPO agent on the Learned World Mode\n",
        "\n",
        "### Overview \n",
        "\n",
        "Now that we have learned a model that simulates the transition function of the real environment, it's time to train an agent on this model. \n",
        "\n",
        "### Instruction \n",
        "\n",
        "In this part, you will learn, visualize, and evaluate a PPO policy on the learned world model, similar to what happened in Part 1, using functions defined in the Helper Function section, Part 1, and Part 2. \n",
        "\n",
        "**Follow instructions to complete each component**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5elpEUg7jv"
      },
      "source": [
        "### 3.1: Train New PPO on learned world environment (4 points)\n",
        "\n",
        "**TODO 1 Instruction**: Initialize a 3-vectorized and a 1-vectorized WorldModelMountainCar environment. \n",
        "\n",
        "**TODO 2 Instruction**: For this part, we will train a separate PPO policy using the learned model environment. This model should be trained with the world model learned in the previous part for 1500000 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1. \n",
        "\n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell: \n",
        "- `real_vec_env_1` \n",
        "- `real_vec_env_3` \n",
        "- `real_env` \n",
        "- `world_vec_env_1` \n",
        "- `world_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az0BzwAihAsJ",
        "outputId": "332e1149-5ed2-4405-8665-1b77a135b291"
      },
      "outputs": [],
      "source": [
        "learner_ckpt_path = 'learner'\n",
        "total_steps = 1500000\n",
        "\n",
        "seed = 937 \n",
        "reseed(seed)\n",
        "\n",
        "# TODO 1: \n",
        "world_vec_env_1 = None\n",
        "world_vec_env_3 = None\n",
        "# END TODO\n",
        "\n",
        "learner_callback = PPOCallback(save_path=learner_ckpt_path, eval_env=world_vec_env_1)\n",
        "\n",
        "# TODO 2: \n",
        "\n",
        "# END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoJ8M-DJPAc6"
      },
      "source": [
        "### 3.2: [PROVIDED] Visualize Policy with Learned World Model and Real Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "BwVfkzCKPAc6",
        "outputId": "c428fae0-5188-46a7-d208-137640c09afe"
      },
      "outputs": [],
      "source": [
        "learner_actor = PPOActor(ckpt=f'{learner_ckpt_path}.zip', environment=world_vec_env_1)\n",
        "visualize(\n",
        "    env_name='WorldModelMountainCar',\n",
        "    algorithm=learner_actor,\n",
        "    env_args={\n",
        "        'world_model': world_model\n",
        "    },\n",
        "    video_name=\"learner\",\n",
        ")\n",
        "\n",
        "visualize(\n",
        "    env_name='MountainCar-v0',\n",
        "    algorithm=learner_actor,\n",
        "    video_name=\"learner_eval\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3: Evaluate Learned Policy (2 points) \n",
        "\n",
        "Evaluate the learner agent on both the learned world model and the MountainCar-v0 environment, take note of the numbers, and analyze them in the write up. \n",
        "\n",
        "**Expected Rewards**: \n",
        "- About -160 to -200 on MountainCar-v0\n",
        "- About -30 to -100 on WorldModelMountainCar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learner_actor = PPOActor(ckpt=f'{learner_ckpt_path}.zip', environment=world_vec_env_1)\n",
        "\n",
        "# TODO: \n",
        "# END TODO: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGMFxLgiPAc6"
      },
      "source": [
        "## Part 4 (Extra Credit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYlxpuOTPAc6"
      },
      "source": [
        "### 4.1: Collect Data with policy trained on world model (1 point)\n",
        "\n",
        "In this section, we collect another 100000 steps of data with the learned agent from the previous part on the real environment, simulating rollouts of the learned agent in real world. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCDp5y61PAc7",
        "outputId": "6a62991d-0e27-4db1-a312-0864a84d8c13"
      },
      "outputs": [],
      "source": [
        "real_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
        "reseed(data_seed, real_env)\n",
        "\n",
        "# TODO: \n",
        "policy_obs, policy_acts, policy_next_obs = None \n",
        "# END TODO\n",
        "\n",
        "visualize_collected_data(policy_obs, policy_next_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsnoInFqPAc7"
      },
      "source": [
        "### 4.2: Aggregate Data (3 point)\n",
        "\n",
        "**Instruction**: Aggregate the expert data collect in Part 2 and the agent data collected from previous section, shuffle their order, and form a new train/val split instances of the `WorldDataset` and dataloaders. \n",
        "- The train / val split should be 80% / 20% "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkVQ7SEhPAc7"
      },
      "outputs": [],
      "source": [
        "# TODO: \n",
        "\n",
        "# END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2sC203aPAc7"
      },
      "source": [
        "### 4.3: Train a second WorldModel using the new aggregated data (3 points)\n",
        "\n",
        "- Train for 50 epochs with lr=0.0001, Adam optimizer, and MSELoss criterion. \n",
        "- Save the train and validation losses across epochs and plot them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2LVh_D5PAc7",
        "outputId": "ee08f559-5aca-4a7c-9fcf-d21e2e160a19"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "reseed(seed)\n",
        "\n",
        "agg_world_model = WorldModel(input_dim=3, hidden_dim_1=16, hidden_dim_2=128, output_dim=2)\n",
        "lr = 0.0001\n",
        "agg_optimizer = torch.optim.Adam(agg_world_model.parameters(), lr=lr)\n",
        "agg_criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "# TODO: \n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the resulting model through expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-ASfx8DPAc7",
        "outputId": "cc53330d-a56f-4e9d-cfc9-b888f33fa27e"
      },
      "outputs": [],
      "source": [
        "# visualize on trained expert\n",
        "visualize(\n",
        "    env_name='WorldModelMountainCar',\n",
        "    algorithm=expert,\n",
        "    video_name='expert_agg_trained_world',\n",
        "    env_args={\n",
        "        'world_model': agg_world_model\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4: Train new PPO policy on world model from 4.3 (4 points)\n",
        "\n",
        "**TODO 1 Instruction**: Initialize a 3-vectorized and a 1-vectorized WorldModelMountainCar environment with the world model trained on aggregated dataset\n",
        "\n",
        "**TODO 2 Instruction**: Train a separate PPO policy using the world model learned on aggregated data. Similar as before, this model should be trained for 1500000 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1. \n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell: \n",
        "- `real_vec_env_1` \n",
        "- `real_vec_env_3` \n",
        "- `real_env` \n",
        "- `world_vec_env_1` \n",
        "- `world_vec_env_3`\n",
        "- `agg_world_vec_env_1` \n",
        "- `agg_world_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_ckpt_path = 'agg_learner'\n",
        "total_steps = 1500000\n",
        "\n",
        "reseed(seed) \n",
        "\n",
        "# TODO 1: \n",
        "agg_world_vec_env_3 = None\n",
        "agg_world_vec_env_1 = None\n",
        "# END TODO \n",
        "\n",
        "policy_callback = PPOCallback(save_path=agg_ckpt_path, eval_env=agg_world_vec_env_1)\n",
        "\n",
        "# TODO : \n",
        "\n",
        "# END TODO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the aggregated PPO Agent on both real environment and learned world environment (2 points)  \n",
        "\n",
        "**Expected Rewards** \n",
        "- About -120 to -150 on MountainCar-v0 \n",
        "- About -100 to -140 on learned environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_learner_actor = PPOActor(ckpt='agg_learner.zip', environment=agg_world_vec_env_3)\n",
        "\n",
        "# TODO: \n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OyawBbGNPAc6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
